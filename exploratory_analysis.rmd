---
output:
  html_document: default
  pdf_document: default
---
# AirBnb

Load required packages.

```{r load packages, warning=FALSE, message=FALSE}

library( tidyverse )
library( dplyr )
library( gridExtra )
library( ggridges )
library( mapview )
library( leaflet )
library( leaflet.extras )
library( scales )
library( stargazer )
library( GGally )
library( corrplot )
library( tm )
library( wordcloud )
library( broom )
library( Hmisc )
library( geosphere )
require( gender )
require( tidytext )
library( ggmap )
library( geosphere )
library( cld2 )
library(knitr)

```

Set up workspace, i.e., remove all existing data from working memory and load data from CSV file.

```{r setup}

rm( list=ls() )
df <- read.csv("./data/airbnb_clean.csv")

```

# Data Overview
The dataset contains information related to Airbnb listings in New York City. Each row in the dataset corresponds to a unique listing with its own identifier and has associated attributes like the type of room, neighborhood details, price, reviews, and the physical coordinates of the listing. Also provided are the name of the listing and the host's name as given on the Airbnb platform as well as how many listings each hosts entertains in total and for how many days each listing is available in a year.

## Dataset Size
```{r}
cat(paste("Total number of listings:", nrow(df)), "\n")
cat(paste("Total number of unique hosts:", length(unique(df$host_id))), "\n")
```
The dataset contains a total of 48,843 listings. These listings come from 37,420 unique hosts. This suggests that while many hosts have a single listing, there's a considerable number of hosts with multiple listings.


## Time Span Covered

```{r}
cat(paste("Earliest review date:", min(df$last_review, na.rm = TRUE)), "\n")
cat(paste("Latest review date:", max(df$last_review, na.rm = TRUE)), "\n")
cat(paste("Time span covered:", 
          as.numeric(difftime(max(as.Date(df$last_review), na.rm = TRUE), 
                              min(as.Date(df$last_review), na.rm = TRUE), units = "days")), "days"), "\n")
```
The dataset spans over a period of 3,024 days, i.e., approximately 8.3 years. The earliest review date on record is March 28, 2011, the latest review is from July 8, 2019. 


## Descriptive Statistics of Continuous Variables
```{r}
# Summary statistics
summary(df[,c('price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365')])

# Price
cat(paste("Average price:", round(mean(df$price, na.rm = TRUE), 2)), "\n")

# Minimum Nights
cat(paste("Average minimum nights:", 
          round(mean(df$minimum_nights, na.rm = TRUE), 2)), "\n")

# Listing Activity Overview
active_listings_past_year <- 
cat(paste("Number of listings with reviews in the past year:", 
          sum(df$last_review > as.Date(df$latest_review) - 365, na.rm = TRUE)), "\n")

# Host Overview
listings_per_host <- table(df$host_id)
multiple_listings_hosts <- sum(listings_per_host > 1)

cat(paste("Number of hosts with multiple listings:", multiple_listings_hosts), "\n")
```

The price of listings varies widely, ranging from $0 listings to luxurious options priced at $10,000. On average, a guest can expect to pay around $152.8 per night, with half of the listings priced at or below $106.

The minimum nights required by hosts shows a broad spread as well, though the typical (median) stay requirement is just 3 nights. On average, listings require stays of about a week (7 nights). This indicates that while most hosts are looking for short to medium-term stays, there are a few outliers.

As for guest feedback, some listings have yet to be reviewed, while others have as many as 629 reviews. The average listing has been reviewed about 23 times. This diversity in reviews shows the variance of listing popularity and guest traffic.

At least 50 % of hosts offer only one listing, with some hosts offering up to 327 apartments in NYC. 

Listings are in part not available for rent at all, the median availability is 45 days out of the 365 days in a year.

## Location
```{r}
cat("Number of unique neighbourhood groups:", 
    length(unique(df$neighbourhood_group)), "\n")

cat(paste("Number of unique neighbourhoods:", 
          length(unique(df$neighbourhood))), "\n")

# Top neighbourhoods
top_neighbourhoods <- head(sort(table(df$neighbourhood), decreasing = TRUE), 5)
cat("Top 5 neighbourhoods by number of listings:\n")
print(top_neighbourhoods)

# Least popular neighbourhoods
bottom_neighbourhoods <- head(sort(table(
  df$neighbourhood), decreasing = FALSE), 5)

cat("Bottom 5 neighbourhoods by number of listings:\n")
print(bottom_neighbourhoods)

# Listings by neighbourhood group
cat("Number of listings by Neighbourhood Group:\n")
table(df$neighbourhood_group)

# Listings by room type
cat("Number of listings by Room Type:\n")
table(df$room_type)

# Bar plot of listings by neighbourhood group:
df %>% 
  filter(!is.na(neighbourhood_group)) %>% 
  ggplot(aes(x = neighbourhood_group)) + 
  geom_bar() + 
  labs(
    title = "Listings by Neighbourhood Group", 
    x = "Neighbourhood Group", 
    y = "Frequency"
  )

# Heat map of listings
ggplot(df, aes(x=longitude, y=latitude)) + 
  geom_density_2d_filled(show.legend = TRUE, aes(fill = ..level..)) + 
  coord_cartesian(xlim = c(-74.08, -73.85), ylim = c(40.64, 40.86)) + 
  labs(title="Density Heatmap of Listings", x="Longitude", y="Latitude")

```

Location plays a vital role in the dataset. The top 5 neighbourhoods feature up to 3,917 listings, while the least popular neighbourhoods only have one listing. Most listings in the dataset are in Manhattan and Brooklyn.

The majority of the listings are for entire homes/apartments (25,393), followed by private rooms (22,306), and a relatively smaller number are shared rooms (1,159).

## Room types
```{r}
cat("Number of unique room types:", length(unique(df$room_type)), "\n")

# Distribution of Room Type
df %>% 
  filter(!is.na(neighbourhood_group)) %>% 
  ggplot(aes(x=room_type)) + 
  geom_bar() + 
  labs(title="Distribution of Room Types", x="Room Type", y="Frequency")

# Geographic distribution of private room vs. apartment
df %>% 
  filter(room_type %in% c('Private room', 'Entire home/apt')) %>% 
  ggplot(aes(x = longitude, y = latitude, color = room_type)) + 
  geom_point() + 
  scale_color_manual(values = c("Private room" = "blue", 
                                "Entire home/apt" = "red")) +
  labs(title = "Geographic Distribution of 'Private Room' vs. 'Entire home/apt'",
       x = "Longitude", 
       y = "Latitude",
       color = "Room Type")

# Bar plot of Room Types by location
df %>% 
  filter(!is.na(neighbourhood_group)) %>% 
  ggplot(aes(x=neighbourhood_group, fill=room_type)) + 
  geom_bar(position="dodge") +
  labs(title="Distribution of Room Types by Neighbourhood Group", 
       x="Neighborhood", 
       y="Frequency")

```

Only few listing are of a shared room. 

Listings for an entire home/apt are more expensive, followed by private and shared rooms.

In Manhattan, most listings are for an entire home/apartment, while in other neighborhoods, private rooms dominate. Brooklyn stands out as a balanced neighborhood with almost equal representation of both types.


## Listing Activity
```{r}
cat(paste("Number of listings with reviews in the past year:", sum(df$last_review > as.Date(df$latest_review) - 365, na.rm = TRUE)), "\n")
```
Out of the entire dataset, 29,118 listings have had a review in the past year. This suggests that a considerable number of listings are active and have had recent guests.

# Transform data

Convert the room_type, neighborhood_group, and neighborhood variables into factors and the last_review variable into a Date object.

```{r}

head(df)
df <- df %>%
  mutate(
    room_type = as.factor(room_type),
    neighbourhood_group = as.factor(neighbourhood_group),
    neighbourhood = as.factor(neighbourhood),
    last_review=as.Date(last_review, format = "%Y-%m-%d")
  )

str(df)

```


# Missing values

Count the number of missing values and calculate the percentage.
 
```{r}

empty_values <- c(NA, NULL, "", " ")

get_na_summary <- function(df) {
  nrows = nrow(df)
  NAs <- data.frame()

  for (column_name in colnames(df)) {
    na_count <- sum(df[[column_name]] %in% empty_values)#is.na(df[[column_name]]))
    row <- data.frame(
      variable = column_name,
      na_count = na_count,
      na_percent = round(na_count / nrows, 4) 
    )
    NAs <- rbind(NAs, row)
  }

  return(NAs)
}

get_na_summary(df)

```

Defining a function to filter out rows where variables contain missing values.
Missing values in reviews_per_month are replaced with 0, assuming no review means zero reviews per month. 
Keep only rows without empty values in the name and host_name variables.
Check the dataframe again for missing values after the cleaning.

```{r}

`%nin%` = Negate(`%in%`)

df <- df %>%
  mutate(reviews_per_month = ifelse(is.na(reviews_per_month), 0, reviews_per_month)) %>%
  filter(name %nin% empty_values) %>%
  filter(host_name %nin% empty_values)

get_na_summary(df)

```

# Feature Engineering

## Review age

Create a variable that adds the most recent review date available in the dataset and store it in latest_review.

```{r}

latest_review <- df %>%
  filter(!is.na(last_review)) %>%
  summarise(max(last_review))
latest_review <- latest_review[1,]

df <- df %>%
  mutate(
    last_review_age = latest_review - last_review,
  )

head(df)

```

## Price bin

Bin price and check the distribution across bins.

```{r}

bins <- c(0, 50, 100, 200, 500, 1000, 10000)
labels <- c("0-50", "51-100", "101-200", "201-500", "501-1000", "1001+")
df <- df %>%
  mutate(price_bin = cut(price, breaks = bins, labels = labels, include.lowest = TRUE, right = FALSE))

df %>%
  count(price_bin) %>%
  ggplot(aes(x = price_bin, y = n)) +
  geom_col() +
  labs(title="Distribution of Price Bins", x="Price Bins", y="Frequency")

```

The listings are distributed across the bins, though there is less data for the two highest price categories.
Most listings are in bins 2 and 3, i.e., between $51 and $500. The adjacent bins 1 and 4 are also well-represented.

## Name length

This adds a column to the dataframe with the count of characters in a listing's name after having removed the spaces: 

```{r}

#count characters (without " ")
df <- df %>%
  mutate(name_length = nchar(str_replace_all(name, " ", "")))

```

## Name Category

Get average number of characters, median, maximum and minimum:

```{r}

avg_name_length <- mean(df$name_length)
median_name_length <- median(df$name_length)
minimum_name_length <- min(df$name_length)
maximum_name_length <- max(df$name_length)

```
The minimum number of characters is 0, which suggests that there are listings without a name. The mean and median are very close (31,76 to 32) which indicates that the distribution could be (close to) symmetrical. The longest name having 151 characters seems to be an outlier. 

Plot number of characters in histogram using bins of size 15: 

```{r}
bin_labels = seq(0, maximum_name_length, 10)
labels <- paste0("  ",bin_labels, "-", bin_labels + 9, "  ")

df %>%
  ggplot( mapping = aes( x=name_length )) +
  geom_histogram(binwidth = 10)  +
  stat_bin(
    binwidth = 10,
    geom = 'text',
    aes(label = after_stat(count)),
    vjust = -0.5,
    size = 3,
    color = "black"
  ) +
  labs( title = "How long are the names of AirBnB listings?",
        x = "Number of characters in listing's name", 
        y = "Frequency") +
  scale_x_continuous(breaks = bin_labels, labels=labels) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
While there are a few outliers (which we look at below), most names fall between 15 and 59 characters.
Quantile berechnen


Look at the longest names (>75) and the shortest (<15): 

```{r}

df %>% filter(name_length > 60)
df %>% filter(name_length < 10)

```
Shorter names not very informational, while the longer names have almost too much information. 
It could make sense to categorize the length into three bins. 


Make length of name into a factor (short, medium, long): 

```{r}
df <- df %>% 
  mutate(name_category = cut(name_length, 
                        breaks = c(0, 9, 59, 151),
                        labels = c("short", "medium", "long"),
                        include.lowest = TRUE)) %>% mutate(name_category = as.factor(name_category))
```

## Upper Case

Another aspect that might be worth investigating is the use of upper case letters. Manual screening showed that there are some names that only consist of upper case letters which can be perceived as "screaming" on the internet. 

Count the number of upper case letters in a listing's name. Then introduce a new categorical (factor) variable that indicates whether a name consists of only upper case letters (1) or not (0) and get some basic summary statistics about it: 

TODO: Remove !,, and such from name length. There could be all-upper-case names that don't appear in list because they contain punctuation :D

```{r}

df <- df %>%
  mutate(upper_case_count = str_count(name, "[A-Z]"))

df <- df %>% 
  mutate(is_upper_case = ifelse(name_length == upper_case_count, 1, 0)) %>% 
  mutate(is_upper_case = as.factor(is_upper_case))

df %>% 
  filter(is_upper_case == 1)

df %>% 
  count(is_upper_case ==1)

```

There are 554 listings where the name contains upper case letters only. 

## Language

Showed some names in a different language. Let's detect them and see if there is an correlation to the number of reviews: 

```{r}
#install.packages("cld3")

df <- df %>% 
  mutate(name_language = detect_language(text = name)) 
df %>% 
  count(name_language)
df$name_language <- df$name_language %>% replace_na('')

```
38 different languages (37 and NA) were detected, majority english. For ease, since new york listing, we concentrate on the difference between english and no english. 

```{r}

df <- df %>% 
  mutate(is_english = ifelse(name_language == "en", 1, 0)) 

```

## Earnings

Create variable indicating listing's average monthly earnings

Note that we assume that every visitor leaves a review as Airbnb's reputation system
incentivizes users to do so. Note also, that we can only calculate
the lowest potential earnings as we only know the minimum number
of nights visitors need to stay.

```{r}

df <- df %>%
  mutate(lowest_monthly_earnings = minimum_nights*reviews_per_month*price)

```

## Distances

```{r}

distance <- function(lat1, long1, lat2, long2) {
  dist <- distGeo(matrix(c(lat1, long1), ncol = 2), matrix(c(lat2, long2), ncol = 2))
  return(dist/1000)
}

manhatten_distance <- function(x1, y1, x2, y2) {
  return(abs(x1 - x2) + abs(y1 - y2))  
}

```

### Distance to center of Manhattan

Create a variable that contains the distance to the "center" of New York in Manhattan, here defined as the Columbus Circle.

```{r}

center_latitude <- 40.767811385445356
center_longitude <- -73.98156481716236

```

```{r}

df <- df %>%
  mutate(
    distance_from_center = manhatten_distance(center_latitude, center_longitude, latitude, longitude)
  )

head(df)

```

### Distance to airports

Build variables with latitude and longitude of several focal points.
We use the three international airports of NY, 3 sigths (Empire State Building, Statue of Liberty, Central Park) and the five centers of every neighbourhood.

```{r}

jfk <- c(latitude = 40.6413, longitude = -73.7781) # Airport jfk
ewr <- c(latitude = 40.6895314, longitude = -74.17446239) # Airport Newark
lga <- c(latitude = 40.7769271, longitude = -73.8739659) # Airport laGuardia

```

Calculate distances between listing and focal point.

```{r}

df <- df %>%
  mutate(
    distance_to_jfk = distance(latitude, longitude, jfk[1], jfk[2]),
    distance_to_ewr = distance(latitude, longitude, ewr[1], ewr[2]),
    distance_to_lga = distance(latitude, longitude, lga[1], lga[2])
  )

```

### Distance to tourist attractions

```{r}

esb <- c(latitude = 40.748817, longitude = -73.985428) # empire state bulding
sol <- c(latitude = 40.689247, longitude = -74.044502) # statue of liberty
cp  <- c(latitude = 40.785091, longitude = -73.968285) # central park

```

```{r}

df <- df %>%
  mutate(
    distance_to_esb = distance(latitude, longitude, esb[1], esb[2]),
    distance_to_sol = distance(latitude, longitude, sol[1], sol[2]),
    distance_to_cp = distance(latitude, longitude, cp[1], cp[2])
  )

```

### Distance to neighbourhood centers

```{r}

c_mh <- c(latitude = 40.776676, longitude = -73.971321) # center Manhattan
c_br <- c(latitude = 40.650002, longitude = -73.949997) # center Brooklyn
c_si <- c(latitude = 40.579021, longitude = -74.151535) # center Staten Island
c_bx <- c(latitude = 40.837048, longitude = -73.865433) # center Bronx
c_qu <- c(latitude = 40.734470, longitude = -73.869720) # center Queens

```

```{r}

df <- df %>%
  mutate(
    distance_to_c_mh = distance(latitude, longitude, c_mh[1], c_mh[2]),
    distance_to_c_br = distance(latitude, longitude, c_br[1], c_br[2]),
    distance_to_c_bx = distance(latitude, longitude, c_bx[1], c_bx[2]),
    distance_to_c_qu = distance(latitude, longitude, c_qu[1], c_qu[2]),
    distance_to_c_si = distance(latitude, longitude, c_si[1], c_si[2])
  )
```

## Log variables

Generate kernel density estimates for each variable in the dataframe to assess their distributions.

```{r}

plot_distributions <- function(df) {
  plots <- lapply(names(df), function(var) {
    ggplot(df, aes(x = df[[var]])) +
      geom_density() + # You can also use geom_histogram() for histograms
      labs(title = var)
  })
  grid.arrange(grobs = plots, ncol = 2)
}

df_sample <- df %>%
  select(latitude, longitude, price, number_of_reviews, reviews_per_month, calculated_host_listings_count,  distance_from_center)

plot_distributions(df_sample)

```

Log transforme skewed distributions.

```{r}

df_log <- df %>%
  mutate(
    log_price = log(price),
    log_number_of_reviews = log(number_of_reviews),
    log_reviews_per_month = log(reviews_per_month),
    log_calculated_host_listings_count = log(calculated_host_listings_count)
  )

df_sample <- df_log %>%
  select(
    latitude, longitude, log_price, log_number_of_reviews, log_reviews_per_month, log_calculated_host_listings_count, distance_from_center
  )

plot_distributions(df_sample)

```

Save the modified data to the dataframe.

```{r}

df <- df_log

```

# Consistency

## Min/max

Summarize the data.

```{r}

df_numeric <- df %>%
  select(
    latitude, longitude, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, 
    calculated_host_listings_count, availability_365, last_review_age, distance_from_center
  )

summary(df_numeric)

```

All numeric variables (except longitude) have non-negative values. Max for availability_365 is 365.


## Price

Price: Minimum value of 0, likely indicating missing data rather than actual free listings.
Minimum Nights: A maximum of 1250 nights suggests potential outliers.

Check entries with price equal to 0.

```{r}

df %>%
  filter(price == 0)

```

Since there are only 11 listings with a price of zero, we assume this is a data gathering error and remove the respective rows.

```{r}

df <- df_non_zero_price <- df %>% 
  filter(price != 0)

```

## Minimum nights

```{r}

max(df$minimum_nights)

```

The maximum for the variable minimum_nights is 1250, meaning that particular listing would have to be rented as a minimum for well over 3 years. This seems excessive as AirBnB in most cases is used for holidays or short to mid stays. We do not see a reason for a host to require the listing to be rented for such a long time and thus will consider it and similar entries as data anomalies.


```{r}

quantile(df$minimum_nights, 0.9999)

```

The minimum required nights is less than 500 for 99.99% of all listing. This number seems more or less reasonable. In the following we will therefore remove all entries that fall above this threshold.

```{r}

df <- df %>%
  filter(minimum_nights <= quantile(minimum_nights, 0.9999))

```


## Unique ids

Compare the number of rows and the number of unique ids to determine whether each row has a unique identifier.

```{r}

length_df <- nrow(df)
unique_ids <- length(unique(df$id))

length_df == unique_ids

```
There are no id inconsistencies.

## Reviews

Filter the dataframe to find entries where reviews_per_month is 0 but number_of_reviews is not 0, find entries with 0 number_of_reviews but a non-NA last_review date and identify rows where reviews_per_month is 0 but last_review is not NA.

```{r}

result <- df %>%
  filter(
    reviews_per_month == 0, 
    number_of_reviews != 0
  )

nrow(result)

result <- df %>%
  filter(
    number_of_reviews == 0,
    !is.na(last_review)
  )

nrow(result)

result <- df %>%
  filter(
    reviews_per_month == 0,
    !is.na(last_review)
  )

nrow(result)

```
All review variables are internally consistent.

# Duplicates

Find duplicates based on the id column and identify listings that are at the exact same location.

```{r}

get_duplicates_by_columns <- function(df, column_names) {
  column_indexes <- unlist(lapply(column_names, function(name) {
    return (grep(name, colnames(df)))
  })) 
  
  return (df[duplicated(df[,column_indexes]) | duplicated(df[,column_indexes], fromLast = TRUE),])
}

```

## Duplicate ids

```{r}

nrow(get_duplicates_by_columns(df, c("id")))

```
## Duplicate names

```{r}

get_duplicates_by_columns(df, c("name")) %>%
  select(id, name, host_name, neighbourhood, price) %>%
  arrange(desc(name))

```
There are a lot of duplicate names in the dataset. Most of the time these listings seem to be from the same host. However, the listings seem to always differ in the neighbourhood or in the price. This will be further analysed in the following.

```{r}

get_duplicates_by_columns(df, c("name", "latitude", "longitude")) %>%
  select(id, name, host_name, price, latitude, longitude) %>%
  arrange(desc(name))

```

As one can see there are no listings with the same name that are also at the same geographic position. Since the combination of name, latitude and longitude is already a super key differentiating every row, no furher supersets of this combination need to be analysed.

## Duplicate positions

```{r}

get_duplicates_by_columns(df, c("longitude", "latitude")) %>%
  select(id, name, host_name, longitude, latitude) %>%
  arrange(desc(latitude))

```
There are some listings that are at the exact same position. However, they more often than not have a different host or name and in every case a different price. Thus, we will not drop these entries and consider them in our analysis.

# Export

```{r}

write.csv(df, "./data/airbnb_clean.csv")

```



# General analysis

## Correlation matrix

To get an overview of the data, explore correlations between variables.

```{r}
# Correlation table with p-values
correlation_data <- df[, c("price", "minimum_nights", "number_of_reviews", 
                           "reviews_per_month", "name_length",
                           "calculated_host_listings_count", "availability_365")]

result <- rcorr(as.matrix(correlation_data))
result$r  # correlation matrix
result$P  # p-values

# Calculate the correlation matrix
cor_matrix <- cor(correlation_data, use = "complete.obs")

# Visualize the correlation matrix using corrplot
corrplot(cor_matrix, method = "color", type = "upper", 
         title = "Correlation Matrix", mar = c(0,0,1,0))

```
All correlations are significant except name_length correlated with minimum_nights and number_of_reviews.

Price:
Exhibits only weak correlations. The slight negative association with number_of_reviews (-0.048) could signal that affordability may marginally boost popularity. Minimal positive correlations are seen with name_length (0.042) and calculated_host_listings_count (0.057), pointing toward  influences on pricing through strategic naming or a host's experience. There is also a small positive link with availability_365 (0.082).

Minimum Nights:
An inverse relationship is noted with number_of_reviews (-0.082) and reviews_per_month (-0.123). This could be due to these apartments having less turnover or being less popular to rent. Positive relationships with availability_365 (0.146) and calculated_host_listings_count (0.131).

Number of Reviews:
The positive correlation with availability_365 (0.172) implies that properties with higher availability tend to garner more reviews. A subtle positive relationship with name_length (0.087), indicating greater popularity of these apartments.

Name Length:
The positive correlation with calculated_host_listings_count (0.152) could indicate that more experienced hosts might employ more detailed names for their listings.

Host Listings Count:
The positive association with availability_365 (0.226) proposes that hosts with a higher listing count tend to offer listings with higher availability. They may be less likely to use their apartments privately, thereby being able to offer them for rent more frequently.


Examine correlations divided by neighborhood.

```{r}
unique_groups <- unique(df$neighbourhood_group)

for (group in unique_groups) {
  
  subset_df <- df[df$neighbourhood_group == group,]
  
  # Subset the data to include only relevant numeric variables
  correlation_data <- subset_df[, c("price", "minimum_nights", "number_of_reviews", 
                                    "reviews_per_month", "name_length",
                                    "calculated_host_listings_count", "availability_365")]

  # Calculate the correlation matrix for the subset
  cor_matrix <- cor(correlation_data, use = "complete.obs")
  
  # Visualize the correlation matrix using corrplot
  corrplot(cor_matrix, method = "color", type = "upper", 
           title = paste("Correlation Matrix for", group), mar = c(0,0,1,0))
}

```

Price:
Notably negative correlation with number_of_reviews especially in Manhattan and Staten Island; positive correlation with name_length only in Manhattan. A nuanced relationship with calculated_host_listings_count across neighborhoods; consistent slight positive with availability, strongest in Manhattan.
    
Minimum Nights:
Uniformly negative correlation with number_of_reviews and reviews_per_month; positive with availability across regions. Generally maintains a positive relationship with calculated_host_listings_count, barring Staten Island.

Number of Reviews:
A consistent positive correlation between number_of_reviews and availability is observed across all neighborhood groups, reaffirming that heightened availability may foster increased reviews. A mild negative correlation with calculated_host_listings_count persists generally, with Staten Island being an exception.

Name Length:
The positive correlation between name_length and calculated_host_listings_count is generally upheld, albeit with Brooklyn diverging from this trend, suggesting variations in naming strategies among hosts with multiple listings.

Calculated Host Listings Count:
A consistent positive relationship with availability across all neighborhoods indicates that hosts with multiple listings might offer increased availability across their properties.

Notably, Manhattan and Staten Island occasionally diverge from a general trend.


## Number of listings by area

```{r}

ggplot(df, aes(x=longitude, y=latitude)) + 
  geom_density_2d_filled(show.legend = TRUE, aes(fill = ..level..)) + 
  coord_cartesian(xlim = c(-74.08, -73.85), ylim = c(40.64, 40.86)) + 
  labs(title="Density Heatmap of Listings", x="Longitude", y="Latitude") + 
  theme_minimal()

# Bar plot to see which neighborhood groups have the most listings:

df %>% 
  filter(!is.na(neighbourhood_group)) %>% 
  ggplot(aes(x = neighbourhood_group)) + 
  geom_bar() + 
  labs(
    title = "Listings by Neighbourhood Group", 
    x = "Neighbourhood Group", 
    y = "Frequency"
  )

```

Most listings in the dataset are in Manhattan and Brooklyn.


## Room types

```{r}
# Distribution of Room Type
df %>% 
  filter(!is.na(neighbourhood_group)) %>% 
  ggplot(aes(x=room_type)) + 
  geom_bar() + 
  labs(title="Distribution of Room Types", x="Room Type", y="Frequency")

# Room Types by Location
df %>% 
  filter(!is.na(neighbourhood_group)) %>% 
  ggplot(aes(x=neighbourhood_group, fill=room_type)) + 
  geom_bar(position="dodge") +
  labs(title="Distribution of Room Types by Neighbourhood Group", 
       x="Neighborhood", y="Frequency")

# Geographic distribution of private room vs. apartment
df %>% 
  filter(room_type %in% c('Private room', 'Entire home/apt')) %>% 
  ggplot(aes(x = longitude, y = latitude, color = room_type)) + 
  geom_point() + 
  scale_color_manual(values = c("Private room" = "blue", "Entire home/apt" = "red")) +
  labs(title = "Geographic Distribution of 'Private Room' vs. 'Entire home/apt'",
       x = "Longitude", 
       y = "Latitude",
       color = "Room Type")

# Price by Room Type
df %>%
  filter(!is.na(log_price) & !is.na(room_type)) %>%
  ggplot(aes(x = room_type, y = log_price)) +
  geom_boxplot() +
  labs(title = "Boxplot of Price vs. Room Type",
       x = "Room Type",
       y = "Log Price")

#Price bins by Room Type
ggplot(df, aes(x=price_bin, fill=room_type)) +
  geom_bar(position="dodge") +   # Use "stack" for a stacked bar plot
  labs(title="Price Bins per Room Type", 
       x="Price Bins", 
       y="Count")

```
Only few listing are for a shared room.

In Manhattan, most listings are for an entire home/apartment, while in other neighborhoods, private rooms dominate. Brooklyn stands out as a balanced neighborhood with almost equal representation of both types.

Listings for an entire home/apt tend to be more expensive, followed by private and shared rooms.


## Price and Location

```{r}
df %>% 
  filter(!is.na(price_bin)) %>% 
  ggplot(aes(x=price_bin)) +
    geom_bar() +
    labs(title="Distribution of Price Bins", x="Price Bins", y="Frequency") +
    facet_wrap(~neighbourhood_group, scales = "free_y")

# Boxplot: Price vs. Neighbourhood Group
df %>%
  filter(!is.na(log_price) & !is.na(neighbourhood_group)) %>%
  ggplot(aes(x = neighbourhood_group, y = log_price)) +
  geom_boxplot() +
  labs(title = "Boxplot of Price vs. Neighbourhood Group",
       x = "Neighbourhood Group",
       y = "Log Price")

# Geographic distribution of price bins
df %>% 
  filter(!is.na(price_bin)) %>% 
  ggplot(aes(x=longitude, y=latitude, color=price_bin)) +
    geom_point(alpha=0.5) +
    scale_color_brewer(palette="Set2") +
    labs(title="Geographic Distribution of Price Bins",
         x="Longitude",
         y="Latitude")

# Geographic distribution of log_price
ggplot(df, aes(x=longitude, y=latitude)) + 
  geom_point(aes(color=log_price), alpha=0.5) +
  scale_color_viridis_c(option = "viridis") +
  labs(title="Heatmap of Prices", x="Longitude", y="Latitude")

```
In the Manhattan area, the majority of listings are priced between $101-200, followed by listings in the $201-500 range. Notably, there are very few listings in the $0-50 price range compared to the other neighborhood groups. Queens predominantly features listings in the $51-100 price range, complemented by a significant number of listings priced between $0-50. Brooklyn has both a lot of 101-200 and 51-100 listings. Note the different scales on the y axis.

Manhattan appears to have the highest median log price among the five neighbourhood groups. It also has the most variability in log prices. Manhattan and Brooklyn also have several outliers, indicating some extremely high-priced listings. Brooklyn and Queens seem to have roughly similar median log prices, though Brooklyn's is slightly higher. Bronx and Staten Island have the lowest median log prices.

Manhattan and Brooklyn seem to be slightly right-skewed, as the whisker on the upper side is longer than the one on the lower side. Bronx, Queens, and Staten Island are more symmetric, as their boxes and whiskers are relatively evenly spread around the median.

The area further to the southwest displays a higher concentration of listings in the $0-50 range. Listings with a price range of $501-1000 are sparsely distributed, with a majority found towards the south, east, and north regions.
    
In southern Manhattan, the predominant hue is green, indicating the steepest prices. Surrounding regions also reflect elevated pricing, demonstrating a gradient effect. As we traverse Brooklyn and Queens, the intensity of green diminishes but remains brighter proximate to Manhattan. Venturing further to the east, north, or south reveals a decline in the average price, signified by progressively more dark green/blue shades.

Since the variation between neighborhoods is high, analyses should be performed separately or they may be confounded by neighborhood_group.

### Distance from center

```{r}
cor_price_distance <- cor.test(df$log_price, df$distance_from_center, method = "pearson")
print(paste("Correlation between log price and distance from center: ", cor_price_distance$estimate))

# Scatter Plot: Price vs. Distance from Center
ggplot(df, aes(x = distance_from_center, y = log_price)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Scatter Plot of Price vs. Distance from Center",
       x = "Distance from Center",
       y = "Log Price")

```

The correlation coefficient of -0.4604 suggests a moderate negative relationship.


## Word Cloud

Which terms are used most often in descriptions?

```{r}
# Create a Corpus
text <- Corpus(VectorSource(df$name))

# Convert all characters to lowercase, remove punctuation and common English stopwords (e.g., "and", "the", "of")
text <- tm_map(text, content_transformer(tolower))
text <- tm_map(text, removePunctuation)
text <- tm_map(text, removeWords, stopwords("en"))

# Create a term-document matrix
# Rows represent terms (words) and columns represent individual entries from df$name. The values in the matrix indicate the frequency of a term in a given document.
tdm <- TermDocumentMatrix(text)

# Calculate word frequencies, sorted in decreasing order
m <- as.matrix(tdm)
word_freqs <- sort(rowSums(m), decreasing=TRUE) 

# Create a data frame with two columns: word (the terms or words) and freq (their respective frequencies)
dm <- data.frame(word = names(word_freqs), freq = word_freqs)

# Create a basic wordcloud
wordcloud(words=dm$word, freq=dm$freq, min.freq=1, max.words=150, random.order=FALSE)

```

Clean up workspace

```{r}

rm(m)
rm(dm)
rm(tdm)
rm(text)

```

# Natural Language Processing

```{r}

model <- aov(df$number_of_reviews ~ df$name_category, data = df)
summary(model)

```
HELP: KANN DAS JEMAND INTERPRETIEREN? ChatGPT sagt signifikant. 

Now we look at a possible correlation between a listing's name length and the number of reviews a listing receives:

```{r}

model <- aov(df$number_of_reviews ~ df$is_upper_case, data = df)
summary(model)

```

Let's see if there is any correlation between the number of reviews and the language of the listing:

```{r}

model_language <- aov(df$number_of_reviews ~ df$is_english, data = df)
summary(model_language)

```
HELP: What does this mean: In other words, there is evidence to suggest that the "is_english" variable is associated with the numeric variable.

## EXPLORING WHAT IS WRITTEN IN NAME

Now we look at what was written in the name of a listing instead of just the "look". 


Get the most common words used in the listing's names: 

```{r}
names <- df$name 
per_line <- data.frame(id=df$id, text = df$name, stringsAsFactors = 
                   FALSE)
new <- per_line %>% unnest_tokens(word, text)
words_per_line <- per_line %>% left_join(new, by = "id")
words_per_line <- words_per_line %>% left_join(df, by = 'id') %>% dplyr::select('id', 'name', 'word')
words_per_line

```
Create a word cloud with the top 100 most commonly used words in names:
(Not very insightful)

```{r}
library(tm)
library(wordcloud)
new %>% count(word, sort = TRUE) %>% with(wordcloud(word, n, max.words = 100))
```

Create a word cloud with the top 100 most commonly used words in names after having removed stopwords:

```{r}
new <-  anti_join(new, get_stopwords())
new %>% count(word, sort= TRUE) %>% with(wordcloud(word, n, max.words = 100))
```
Count the most commonly used positive words:

```{r}
#install.packages("textdata")
library(textdata)
positive <- get_sentiments("nrc") %>%
  filter(sentiment == "positive")

new %>%
  semi_join(positive) %>%
  count(word, sort = TRUE)
```
Count the most commonly used negative words: 

```{r}
negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

new %>%
  semi_join(negative) %>%
  count(word, sort = TRUE)
```
Show the top 150 words by sentiment: 

```{r}
sentiments <- get_sentiments("nrc")


library(wordcloud2)
top_150 <- new %>% count(word, sort= TRUE) %>%
  left_join(sentiments) %>% 
  mutate(sentiment = ifelse(is.na(sentiment), "neutral", sentiment)) %>% 
  mutate(color = ifelse(sentiment == "positive", "green", 
                       ifelse(sentiment == "negative", "red", "grey"))) %>% slice(1:100) 



wordcloud2(top_150, size=1, color=top_150$color)


```


Get the "most positive" listings:

```{r}
total_name_sentiment <- words_per_line %>% left_join(sentiments) %>% mutate(sentiment = ifelse(is.na(sentiment),"neutral", sentiment)) %>% group_by(id) %>%summarise(sentiment_total = sum(sentiment =="positive")) %>% arrange(desc(sentiment_total))
total_name_sentiment
```
See if correlation of positive listing with availability, number of reviews, reviews per motnh
No 

```{r}
df <- left_join(df, total_name_sentiment, by="id")
cor(df$sentiment_total, df$reviews_per_month)
```

## MULTIPLE HOSTINGS 

Hat das schon jemand? -> Dann hier 

By looking at the data, it seems that sometimes multiple hosts are behind one host_id. There are various ways to identify them. We assume that there is more than one host, if one of the following appears in a host name: "+" , "/", " and ", "&". To make sure we do not include names like "Andrea",  we search for " and " with leading and trailing spaces.

```{r}
#adds logical column that indicates whether name contains " and ", "+", "/" or "&" ; ignore.case makes sure that all variations of " and " are covered
df$is_more_than_one_host <- grepl(" and |&|\\+|/", df$host_name, ignore.case = TRUE)
#adds column that contains vector of all names if is_more_than_one_host is true (more than one host)
df$host_names_all <- ifelse(df$is_more_than_one_host == TRUE, strsplit(df$host_name, "(?i) and |&| & |\\+| \\+ |/| / ", perl=TRUE), c(""))
#count rows where this is the case
nrow(df[df$host_names_all != '', ])
```

TODO: GROUP BY HOST ID TO GET NUMBER OF HOSTS WITH MORE THAN ONE 
At least 1852 listings appear to have more than one host. This does not necessarily mean that the rest have only a single host. Considering that companies also list their accommodations on Airbnb, this probably rather represents the minimum. The presence of hosts associated with companies is not part of this analysis and needs further investigations. 

Short look at how many distinct hosts this is: 

```{r}
df %>% filter(is_more_than_one_host == TRUE) %>% summarise(count = n_distinct(host_id))
```
There are 1229 distinct host_ids.

## GENDER ANALYSIS

Given the availability of host names, it might be insightful to look at the gender of the hosts and how the gender relates to other variables. 
We will use the gender package (https://github.com/lmullen/gender) to assign a gender classification to each host name. 
The package uses data from the Social Security Administration, the U.S. Census Bureau (via IPUMS USA), and the North Atlantic Population Project. 
We are aware that this approach and therefore the results from this whole section can only be of limited significance and should only be taken as indicator that this might be something worth investigating.

Before we do this, we need to split the values in the column that contains all names and create a row for each name:  

```{r}
#add a new column called "x", filter for all values in "host_names_all" where the list contains more than one value (meaning more than one host) and use unnest to make a separate row for each name in "host_names_all" then rename "host_names_all" to "host_names_splitted" and call "x" "host_name_all" instead
df <- df %>% mutate(x = host_names_all) %>% filter(lengths(host_names_all) > 0) %>% 
  unnest(host_names_all) %>% rename(host_names_splitted = host_names_all) %>% rename(host_names_all = x)
#add names where empty to splitted column so the original is untouched
df$host_names_splitted[!df$is_more_than_one_host] <- df$host_name[!df$is_more_than_one_host]
#ensure df stays a data drame
df <- as.data.frame(df)
df
```


Now we can assign the sex/gender to each name, using the "gender" package mentioned above. We do this in chunks, to speed up the process. 

```{r}
chunksize <- 100 
#two temporary variables 
min <- 1
max <- 100
#create temporary dataframe
gender <- data.frame()
for(i in 1:ceiling(nrow(df)/chunksize)){
  #check if max is higher than the number of rows in the dataframe to make sure slicing in next line works correctly
  max <- ifelse(max>nrow(df), nrow(df), max)
  #select next 100 rows
  row_slice_100 <- df[min:max,]
  #use gender function to assign gender
  host_names_100_gender <- gender(row_slice_100$host_names_splitted, year=2010, method="ssa")
  #rename the name column that was automatically created by the function gender to make sure it can be joined in the next line and remove duplicate columns
  host_names_100_gender <- host_names_100_gender %>% rename(host_names_splitted = name) %>% distinct(host_names_splitted, .keep_all = TRUE)
  #join the results from the use of the gender function with the slice of the data frame and drop columns that were automatically added by the function "gender" 
  host_names_gender <- row_slice_100 %>% left_join(host_names_100_gender, by = "host_names_splitted") %>%  dplyr::select(-c(proportion_male, proportion_female, year_min, year_max))
  
  #add 100 to min and max to select next 100 rows in next iteration
  min <- min+100
  max <- max+100
  #add rows to temporary df gender
  gender <- rbind(gender, host_names_gender)
}
#assign gender to df and make sure it is a data frame
df <- as.data.frame(gender)

#remove all temporary used variables and the temporary data frame
rm(gender, min, max, chunksize, i, row_slice_100, host_names_100_gender, host_names_gender)

df$gender <- as.factor(df$gender)
```

Let's see if there are hosts with even more than two names by counting all the names associated with a listing id:

```{r}
df <- df %>%
  group_by(id) %>%
  mutate(host_count = n()) %>%
  as.data.frame()

max(df$host_count)

```

There seems to be a few listings with more than even two hosts. Looking at the data there is one with 4 hosts and one with 3. 


We now take a closer look at the "combined" hosts. We only want to assign one gender per host_id, so that we have the same number of rows as in the original dataset. This means we want to categorize into three categories: male, female, mixed.

```{r}
# create temporary data frame and filter for rows where there is more than one host and then group by host_id and check if gender values are the same and assign the right value
result_df <- df %>%
  filter(is_more_than_one_host == TRUE) %>%
  group_by(host_id) %>%
  summarise(result = if (length(unique(gender)) == 1) unique(gender) else "mixed")

# Merge the result back into the original data frame
df <- left_join(df, result_df, by = "host_id")
df <- df %>%
  mutate(gender = case_when(
    !is.na(result) ~ result,
    TRUE ~ gender  # Keep the original value when the condition is not met
  )) %>% dplyr::select(-c(host_names_splitted, result)) %>% distinct(.keep_all =  TRUE)

rm(result_df)
```


We will now take a look at the gender distribution of the listings in our dataset. Note that hosts can appear more than once. 

```{r}
#counts by gender
gender_counts <- df %>%
  group_by(gender) %>%
  summarise(count = n())

#sum of all entries
total_count <- sum(gender_counts$count)

# Creates a pie chart
ggplot(gender_counts, aes(x = "", y = count, fill = gender)) +
  geom_bar(stat = "identity") +
  coord_polar("y") +
  labs(title = "Gender Distribution") +
  theme_void() + 
  geom_text(aes(label = paste0(gender, "\n", count, " (", scales::percent(count / total_count), ")")), 
            position = position_stack(vjust = 0.5))

#remove help variables and dfs 
rm(gender_counts, total_count)
```
We can see a slight overrepresentation of female names, but with 20% of the names remaining unidentified this has only limited significance.Next, we will investigate these unidentified names.
Another interesting insight is that most hosts that are more than one person seem to be mixed. Remember 1852 from above and now we see 1516 hosts of mixed gender. 

---------- Bis hier ist alles ready ------------



Investigate N/A values. We do 
TODO: Wordcloud

```{r}
host_names_no_gender <- df %>% filter(is.na(gender))
#host_names_no_gender <- df %>% distinct(host_name_splitted) TODO: @Milena host_name_splitted exisitert nicht 
host_names_no_gender
#host_count <- df%>% count(host_name_splitted, sort = TRUE)
#host_count
```



Remove N/A values: 

```{r}
host_names_gender <- df %>% filter(!is.na(gender)) # TODO: @Milena ist das richtig so?

gender_counts <- host_names_gender %>%
  group_by(gender) %>%
  summarise(count = n())

total_count <- sum(gender_counts$count)

# Create a pie chart
ggplot(gender_counts, aes(x = "", y = count, fill = gender)) +
  geom_bar(stat = "identity") +
  coord_polar("y") +
  labs(title = "Gender Distribution") +
  theme_void() + 
  geom_text(aes(label = paste0(gender, "\n", count, " (", scales::percent(count / total_count), ")")), 
            position = position_stack(vjust = 0.5))
```

number of listings male vs female:

```{r}

```

more than one listing male vs female:

```{r}
more_than_one_listing <- host_names_gender %>% group_by(host_id) %>%summarise(total_listing = sum(n())) 
unique_listing_counts <- more_than_one_listing %>%
  distinct(host_id, total_listing)
merged_data <- host_names_gender %>% left_join(unique_listing_counts, by="host_id") %>% filter(total_listing > 1) %>% distinct(host_id, host_name, gender, total_listing)
merged_data
```


male vs female neighbourhoods:
```{r}
#df_counts <- df_counts %>% # TODO: @Milena df_counts existiert nicht und wenn ich es mit df ersetzt funktioniert es nicht
#  group_by(neighbourhood_group) %>%
#  mutate(percentage = count / sum(count) * 100)

# Create the stacked bar chart
#p <- ggplot(df_counts, aes(x = neighbourhood_group, y = percentage, fill = gender)) +
#  geom_bar(stat = "identity") +
#  labs(
#    title = "Listings by Neighborhood and Gender",
#    x = "Neighborhood",
#    y = "Percentage"
#  )

# Annotate the bars with percentages adding up to 100%
#p + geom_text(aes(label = scales::percent(percentage / 100)), position = position_stack(vjust = 0.5))
```

Boxplot number of reviews by gender:
TODO: Add a pyramid chart (wie oft demographics in einem Land dargestellt werden)

```{r}
ggplot(host_names_gender, aes(x = gender, y = number_of_reviews)) +
  geom_boxplot() +
  labs(
    title = "Number of Reviews by Gender",
    x = "Gender",
    y = "Number of Reviews"
  )

ggplot(host_names_gender, aes(x = gender, y = number_of_reviews)) +
  geom_violin() +
  labs(
    title = "Number of Reviews by Gender (Violin Plot)",
    x = "Gender",
    y = "Number of Reviews"
  )

```

boxplot price by gender

```{r}
ggplot(host_names_gender, aes(x = gender, y = price)) +
  geom_boxplot() +
  labs(
    title = "Price by Gender",
    x = "Gender",
    y = "Price"
  )

ggplot(host_names_gender, aes(x = gender, y = price)) +
  geom_violin() +
  labs(
    title = "Number of Reviews by Gender (Violin Plot)",
    x = "Gender",
    y = "Number of Reviews"
  )
```

Boxplot by calculated_host_listings_count

```{r}
ggplot(host_names_gender, aes(x = gender, y = calculated_host_listings_count)) +
  geom_boxplot() +
  labs(
    title = "calculated_host_listings_count by Gender",
    x = "Gender",
    y = "calculated_host_listings_count"
  )
```

# Earnings analysis

## Data Exploration

Examine summary statistics including range, and overview distribution of earnings

```{r}

# gather summary statistics
summary(df$lowest_monthly_earnings)

max(df$lowest_monthly_earnings) - min(df$lowest_monthly_earnings)

#draw histogram of monthly earnings
df %>%
  ggplot(aes(lowest_monthly_earnings)) +
  geom_histogram() +
  scale_y_continuous()

```

As the distribution is extremely skewed to the right (median is 124.7, while range is 202783.7), we conduct a log-transformation.


```{r}

# log-transform earnings
df <- df %>%
  mutate(log_earnings = log(lowest_monthly_earnings))

# check for infinite values and how they correlate to original earnings variable
inf_earnings <- df %>%
  filter(is.infinite(log_earnings)) %>%
  select(log_earnings, lowest_monthly_earnings)

summary(inf_earnings$lowest_monthly_earnings)

inf_earnings %>%
  summarise(mean_earnings = mean(lowest_monthly_earnings),
            count = n())


```

After log-transformation, 10029 observations have been assigned infinite values.
This is, because their lowest monthly earnings are 0.
To further investigate the earnings distribution, we exclude these listings.

```{r}

# filter out observations with 0 earnings
earnings_df <- df %>%
  filter(lowest_monthly_earnings > 0)

```

We calculate basic and additional summary statistics to display the distribution in a meaningful way.

```{r}
# store summary statistics of earnings distribution
# estimate upper whisker
IQR <- IQR(earnings_df$log_earnings)
calc_upper_whisker <- (quantile(earnings_df$log_earnings, probs = 0.75) + IQR*1.5)
select_upper_whisker <- earnings_df$log_earnings >= calc_upper_whisker
upper_whisker <- earnings_df$log_earnings[select_upper_whisker]
upper_whisker <- min(upper_whisker)

# estimate lower whisker
calc_lower_whisker <- (quantile(earnings_df$log_earnings, probs = 0.25) - IQR*1.5)
select_lower_whisker <- earnings_df$log_earnings <= calc_lower_whisker
lower_whisker <- earnings_df$log_earnings[select_lower_whisker]
lower_whisker <- max(lower_whisker)

# upper and lower quantile and median
earnings_distribution <- c(
  min(earnings_df$log_earnings), 
  quantile(earnings_df$log_earnings, probs = c(0.25, 0.5, 0.75)),
  max(earnings_df$log_earnings)
)

#draw histogram
earnings_df %>%
  ggplot(aes(x = log_earnings)) +
  geom_histogram() +
  labs(title="Earnings Distribution") +
  xlab("Logged Earnings") +
  ylab("Frequency") 

# draw boxplot showing delogged values
earnings_df %>%
  ggplot(aes(y = log_earnings)) +
  geom_boxplot() +
  stat_boxplot(geom = "errorbar") +
  scale_y_continuous(breaks = c(earnings_distribution, lower_whisker, upper_whisker), 
                     labels = as.integer(exp(c(earnings_distribution, lower_whisker, upper_whisker)))) +
  scale_x_continuous(breaks = NULL, labels = NULL) +
  labs(
    y = "Lowerst Monthly Earnings (De-Logged)",
    title = "Earnings Distribution"
  )

# violin plot by neighbourhood groups
earnings_df %>%
  ggplot(aes(x = reorder(neighbourhood_group, +lowest_monthly_earnings), y = log_earnings)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75), trim = TRUE) +
  labs(
    title = "Monthly Earnings Distribution by Neighbourhood Group",
    y = "Logged Lowest Monthly Earnings",
    x = NULL
  )

```

The histogram shows that the distribution of monthly earnings is approximately normally distributed. We continue to see, however, that more listings belong to the lower half of the distribution.

50% of listings earned up to 225 per month. 75% of listings earned up to 612 per month.
The Interquantile-Range corresponds to 550. Particularly, the middle half of the distribution earned between 62 and 612 per month.
There is much more variance in monthly earnings in the upper 25% of the distribution.
Excluding outliers, the range between the weakest and strongest earning listing in the upper 25% of the distribution is 18738.
As seen in the original boxplot, some outliers have extremely high earnings, with the maximal earning per month being 202783.
Considering providing an Airbnb listing a form of income, the highest percentiles of the distribution seem to account for a very high inequality in earnings. 

The violin plots convey, that there is less variance in monthly earnings in the Bronx and Staten Island and that Brooklyn and Manhattan account for the abovementioned outliers.

We will now calculate the lowest monthly earnings of NYC as a whole.

```{r}

# sort data based on earnings
earnings_df <- earnings_df[order(earnings_df$lowest_monthly_earnings), ]

# Calculate cumulative earnings
earnings_df$cumulative_earnings <- cumsum(earnings_df$lowest_monthly_earnings)

# view NYC's monthly earnings
cumulated_earnings <- max(earnings_df$cumulative_earnings)
cumulated_earnings

```

Cumulating the lowest monthly earnings of Airbnbs in New York since 2011, the whole city earns at least 25.793.569 per month.

Taking the identified skewness as a point of departure, we will investigate the shares, the parts of the distribution have on this sum.

```{r}

# cut total monthly earnings in half
cumulated_earnings_0.5 <- cumulated_earnings / 2
cumulated_earnings_0.5

# identify percentile at which cumulated monthly earnings are equal
q_earnings_halfcut <- ecdf(earnings_df$cumulative_earnings)(cumulated_earnings_0.5)
q_earnings_halfcut

```

The percentile at which the cumulative monthly earnings of both halfs is equal is approximately 0.94

```{r}

# create vector indicating position in earnings distribution
q_value_halfcut <- quantile(earnings_df$lowest_monthly_earnings, probs = q_earnings_halfcut)
quantile_indicator <- ifelse(earnings_df$lowest_monthly_earnings > q_value_halfcut , "upper 6%", "lower 94%")
earnings_df <- cbind(earnings_df, quantile_indicator)

```

Having done the calculation, we continue with description.
We draw a piechart.

```{r}

# prepare data for plotting
earningsplot_df <- earnings_df %>%
  group_by(quantile_indicator) %>%
  summarise(
    tot_earnings_mio = (sum(lowest_monthly_earnings)/1000000),
    mean_earnings = mean(lowest_monthly_earnings),
    mean_price = mean(price),
    mean_monthly_reviews = mean(reviews_per_month),
    mean_min_nights = mean(minimum_nights),
    mean_review_count = mean(number_of_reviews),
    mean_review_age = mean(last_review_age),
    count = n()
    )

# draw piechart
earningsplot_df %>%
  ggplot(aes(x = "", y = tot_earnings_mio, fill = quantile_indicator)) +
  geom_bar(stat = "identity",
           show.legend = FALSE) +
  coord_polar("y") +
  labs(title = "Cumulative Monthly Earnings of upper 6% and lower 94%") +
  theme_void() + 
  geom_text(aes(label = paste0(quantile_indicator, "\n",
                               sprintf("%.1f", tot_earnings_mio), " Mio",
                               "\n", "n = ", count)), 
            position = position_stack(vjust = 0.5)
            )

```

We also want to (in general terms at least) identify what is causing the high earnings.
Thus, we look at mean statistics on earnings-determining variables.

```{r}

# prepare data for table comparison
earningstable_df <- earningsplot_df %>%
    column_to_rownames(var = "quantile_indicator")
earningstable_df$mean_review_age <- as.numeric(earningstable_df$mean_review_age)
earningstable_df <- t(earningstable_df)
earningstable_df <- as.data.frame(earningstable_df)
earningstable_df$`lower 94%` <- as.numeric(earningstable_df$`lower 94%`)
earningstable_df$`upper 6%` <- as.numeric(earningstable_df$`upper 6%`)
earningstable_df <- earningstable_df %>%
  mutate(q6_relative_to_q94 = `upper 6%` / `lower 94%`)
earningstable_df <- round(earningstable_df, 2)

# create statistics table
rownames(earningstable_df) <- c("Lowest Monthly Earnings",
                                "Mean Monthly Earnings",   
                                "Mean Price",
                                "Mean Monthly Reviews",
                                "Mean Minimum Nights",
                                "Mean Review Count",
                                "Mean Age of Last Review",
                                "n")
colnames(earningstable_df) <- c("lower 94%",
                                "upper 6%",
                                "Proportion")
earningstable <- knitr::kable(earningstable_df, 
      caption = "Statistics on Monthly Earnings Distribution Shares", )
earningstable

```

Accumulated, the highest 6% in the earnings distribution earn as much as the 96% below them.
Expressed in total numbers, the highest-performing 2388 listings generated monthly earnings equal to the remaining 36417 listings. 

Examining average characteristics determining earnings conveys that hosts may profit from a high utilization rate.
To be more precise, although the price of highest-performing listings is 2.6 times higher compared to the average price of the remaining listings, price alone may not explain the drastic difference in monthly earnings. Rather, it is the combination of price and utilization indicators that cause higher earnings. On average, a listing of the highest-performing share in the distribution entertains almost twice as much visitors per month, and has had 1.5 times more visitors in total, while visitors are required to stay 4.4 times as long. In addition, for an average high-performing listing, only 81 days have past since the last review, whereas for an average listing of the remaining share 289 days have past. Thus, we may assume that that a listing in the highest 6% of the monthly earnings distribution has less gaps between visitors, resulting in a higher occupation rate.

As we end the analysis of earnings, we clean up the workspace.

```{r}

# clear workspace
rm(inf_earnings,
   earnings_df,
   IQR,
   calc_upper_whisker,
   select_upper_whisker,
   upper_whisker,
   calc_lower_whisker,
   select_lower_whisker,
   lower_whisker,
   earnings_distribution,
   cumulated_earnings,
   cumulated_earnings_0.5,
   q_earnings_halfcut,
   q_value_halfcut,
   quantile_indicator,
   earningsplot_df,
   earningstable_df,
   earningstable,
   inf_earnings,
   earnings_df,
   IQR,
   calc_upper_whisker,
   select_upper_whisker,
   upper_whisker,
   calc_lower_whisker,
   select_lower_whisker,
   lower_whisker,
   earnings_distribution,
   cumulated_earnings,
   cumulated_earnings_0.5,
   q_earnings_halfcut,
   q_value_halfcut,
   quantile_indicator,
   earningsplot_df,
   earningstable_df,
   earningstable
)

```


# Geographic analysis

## Distances

First we want to observe the correlation between price per night and the distance to the biggest airport jfk.
```{r}

plot(df$distance_to_jfk, df$price)

```
Since their are a few outliers in price, we can either use the variable log_price or use the 99pct quartil of the variable price. 
Furthermore there seem to be listings > 30km away from jfk. Those will be removed as well.
```{r}

price_pct99 <- quantile(df$price, probs = 0.99)

#create new dataframe where outliers are removed
df_2 <- df %>% filter(price < price_pct99) %>% filter(distance_to_jfk < 30)

```

```{r}

plot(df_2$distance_to_jfk, df_2$price)

```
Now we want to proceed with a linear regression.
```{r}
lin_model_jfk <- lm(price ~ distance_to_jfk, data = df_2)
summary(lin_model_jfk)

plot(df_2$distance_to_jfk, df_2$price, xlab = "Distance to JFK (in kilometers)", ylab = "Price per night (in $)", main = "Regression anaylsis (distance to JFK vs price per night)")
abline(lin_model_jfk, col = "red")
```
A significant correlation between distance to JFK and price per night (ppn) can be observed. The ppn increases around 7$ with every kilometer distance, c.p.. This result is intuitve since apartments around the airport i.e. airport hotels are always cheaper. Furthermore apartments nearer to the airport could be less expensive due to external factors like the noise of planes. 
In further discussions can be observed, if the outliers > 30km away from JFK could have a negative impact on the price, i.e. these apartments are to far away from center points of NY.

Now we conclued with the same anaylsis, but for all three airports.
```{r}
lin_model_airports <- lm(price ~ distance_to_jfk + distance_to_ewr + distance_to_lga, data = df_2)
summary(lin_model_airports)
```
Lets check for correlation between the price and the distance to NY sights.
```{r}
lin_model_sights <- lm(price ~ distance_to_esb + distance_to_sol + distance_to_cp, data = df_2)
summary(lin_model_sights)
```
The impact of sights is always significant. The estimate for esb is negative which is intuitve, because in the surounding area the rent is high, which leads to higher ppn in the listed airbnbs. Moving away from the center has therefore negative impact on prices. 

Check, if we can increase quality of model with adding variables.
```{r}
#lin_model2 <- lm(price ~ distance_to_jfk + neighbourhood_group + room_type + number_of_reviews, data = df_reduced2)
#summary(lin_model2)


#plot(df_reduced$distance_to_jfk, df_reduced$price, xlab = "Distance to #JFK (in meters)", ylab = "Price per night (in $)", main = "Regression #anaylsis (df_reduced)")
#abline(lin_model, col = "red")
```

Lets investigate if other focal points can express similar results.

```{r}

plot(df$distance_to_ewr, df$price)

```
For completeness: Do same with LGA Airport

```{r}

plot(df$distance_to_lga, df$price)

```

Not only airports can be focal points, sights might be also from interest. Therefore we look for the coordinates of statue of liberty, central park, empire state building.

```{r}
price_pct99 <- quantile(df$price, probs = 0.99)
df_reduced2 <- df %>% filter(price < price_pct99)

plot(df_reduced2$distance_to_sol, df_reduced2$price)
```
```{r}

lin_model3 <- lm(price ~ distance_to_sol, data = df_reduced2)
summary(lin_model3)


plot(df_reduced2$distance_to_sol, df_reduced2$price, xlab = "Distance to statue of liberty (in meters)", ylab = "Price per night (in $)", main = "Regression anaylsis (df_reduced2)")
abline(lin_model3, col = "red")

```

```{r}

price_pct99 <- quantile(df$price, probs = 0.99)
df_reduced2 <- df %>% filter(price < price_pct99)

plot(df_reduced2$distance_to_esb, df_reduced2$price)

```

```{r}
lin_model4 <- lm(price ~ distance_to_esb, data = df_reduced2)
summary(lin_model4)


plot(df_reduced2$distance_to_esb, df_reduced2$price, xlab = "Distance to empire state bulding (in kilometers)", ylab = "Price per night (in $)", main = "Regression anaylsis (df_reduced2)")
abline(lin_model3, col = "red")
```

```{r}
lin_model5 <- lm(price ~ distance_to_cp, data = df_reduced2)
summary(lin_model5)


plot(df_reduced2$distance_to_cp, df_reduced2$price, xlab = "Distance to central park (in kilometers)", ylab = "Price per night (in $)", main = "Regression anaylsis (df_reduced2)")
abline(lin_model5, col = "red")
```

```{r}
df_reduced2$sq_distance_to_esb <- df_reduced2$distance_to_esb^2

sq_model5 <- lm(price ~ distance_to_esb + sq_distance_to_esb, data = df_reduced2)
summary(sq_model5)
```
```{r}
values <- seq(0, 25, 0.1)

pred <- predict(sq_model5, list(distance_to_esb = values, sq_distance_to_esb = values^2))

plot(df_reduced2$distance_to_esb, df_reduced2$price, xlab = "Distance to esb (in kilometers)", ylab = "Price per night (in $)", main = "Regression anaylsis (df_reduced2)")
lines(values, pred, col = "blue")

```
```{r}
df_reduced2$sq_distance_to_esb <- df_reduced2$distance_to_esb^2
df_reduced2$poly_distance_to_esb <- df_reduced2$distance_to_esb^3

poly_model5 <- lm(price ~ distance_to_esb + sq_distance_to_esb + poly_distance_to_esb, data = df_reduced2)
summary(poly_model5)

values <- seq(0, 25, 0.1)

pred <- predict(poly_model5, list(distance_to_esb = values, sq_distance_to_esb = values^2, poly_distance_to_esb = values^3))

plot(df_reduced2$distance_to_esb, df_reduced2$price, xlab = "Distance to esb (in kilometers)", ylab = "Price per night (in $)", main = "Regression anaylsis (df_reduced2)")
lines(values, pred, col = "blue")

```

Now, use all calculated distances to sights to make a regression with on price.
```{r}
lin_model6 <- lm(price ~ distance_to_esb + distance_to_sol + distance_to_cp + neighbourhood_group, data = df_reduced2)
summary(lin_model6)


#plot(df_reduced2$distance_to_cp, df_reduced2$price, xlab = "Distance #to central park (in kilometers)", ylab = "Price per night (in $)", #main = "Regression anaylsis (df_reduced2)")
#abline(lin_model5, col = "green")
```
```{r}
bins <- c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 500)
labels <- c("0-5", "6-10", "11-15", "16-20", "21-25", "26-30", "31-35", "36-40", "41-45", "46-50", ">50")

df$distance_esb_bin <- cut(df$distance_to_esb, breaks = bins, labels = labels, include.lowest = TRUE, right = FALSE)

df_reduced2$distance_esb_bin <- cut(df_reduced2$distance_to_esb, breaks = bins, labels = labels, include.lowest = TRUE, right = FALSE)



table(df$distance_esb_bin)
table(df_reduced2$distance_esb_bin)

```
```{r}
df %>%
  group_by(distance_esb_bin) %>%
  summarise(mean = mean(price))
```
```{r}
ggplot(df_reduced2, aes(x=distance_esb_bin, y=price)) + 
  geom_boxplot()+
  stat_summary(fun.y=mean, geom="point", 
               shape=20, size=4, color="red", fill="red")
```



## Neighbourhood group distribution

```{r}

neighbourhood_group_counts <- df %>%
  count(neighbourhood_group)

population_df <- data.frame(
  neighbourhood_group = c("Manhattan", "Brooklyn", "Queens", "Staten Island", "Bronx"),
  population = c(1600000, 2500000, 2271000, 475000, 1427000)
)

area_df <- data.frame(
  neighbourhood_group = c("Manhattan", "Brooklyn", "Queens", "Staten Island", "Bronx"),
  area = c(59, 180, 280, 152, 110)
)

listing_count_pie_chart <- ggplot(neighbourhood_group_counts, aes(x = "", y = n, fill = neighbourhood_group)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start = 0) +
    ggtitle("Number of listings per neighbourhood group") +
    theme_void()

population_pie_chart <- ggplot(population_df, aes(x = "", y = population, fill = neighbourhood_group)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start = 0) +
    ggtitle("Population per neighbourhood groups") +
    theme_void()

area_pie_chart <- ggplot(area_df, aes(x = "", y = area, fill = neighbourhood_group)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start = 0) +
    ggtitle("Area per neighbourhood groups") +
    theme_void()

plots <- list(
  listing_count_pie_chart,
  population_pie_chart,
  area_pie_chart
)

grid.arrange(grobs = plots, padding = 2, ncol = 2)

```

```{r}

ggplot(df, aes(x = distance_from_center, y = price, color = neighbourhood_group)) +
  geom_point() +
  theme_minimal()

```



```{r}

ggplot(df, mapping = aes(y = log_price, x = neighbourhood_group)) +
  geom_boxplot()

```

```{r}

df %>%
  filter(log_price >= 0) %>% 
  ggplot(mapping = aes(x = log_price, y = neighbourhood_group)) +
  geom_density_ridges(alpha = 0.5)

```

## Most expensive neighbourhoods

```{r}

get_most_expensive_neighbourhoods_in_group <- function(group) {
  most_expensive_neighbourhoods <- df %>%
    filter(neighbourhood_group == group) %>%
    group_by(neighbourhood) %>%
    summarise(sum_price = sum(price)) %>%
    arrange(desc(sum_price))
  
  df %>%
    filter(neighbourhood_group == group) %>%
    group_by(neighbourhood, room_type) %>%
    summarise(sum_price = sum(price)) %>%
    arrange(match(neighbourhood, most_expensive_neighbourhoods$neighbourhood)) %>%
    mutate(neighbourhood = as.factor(neighbourhood)) %>%
    head(20) %>%
    ggplot(mapping = aes(x = factor(neighbourhood, level = most_expensive_neighbourhoods$neighbourhood), y = sum_price, fill = room_type)) +
    geom_col(stat = "identity") +
    labs(
      title = paste("Most expensive neighbourhoods in", group),
      x = "Neighbourhood",
      y = "Price in $"
    )
}

```

### Queens

```{r}

get_most_expensive_neighbourhoods_in_group("Queens")

```

### Brooklyn

```{r}

get_most_expensive_neighbourhoods_in_group("Brooklyn")

```

### Manhattan

```{r}

get_most_expensive_neighbourhoods_in_group("Manhattan")

```

### Staten Island

```{r}

get_most_expensive_neighbourhoods_in_group("Staten Island")

```

### Bronx

```{r}

get_most_expensive_neighbourhoods_in_group("Bronx")

```

### Queens

```{r}

get_most_expensive_neighbourhoods_in_group("Queens")

```

## Geographic Listing density

```{r}

icon_width <- 30
icon_height <- 30
icon_base_url <- "https://img.icons8.com/?size=2x&format=png&id="

statue_of_liberty_icon <- makeIcon(
  iconUrl = paste(icon_base_url, 4674),
  iconWidth = icon_width, iconHeight = icon_height
)

skyscraper_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "cGbCkzI99Cf8"),
  iconWidth = icon_width, iconHeight = icon_height
)

park_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "2410"),
  iconWidth = icon_width, iconHeight = icon_height
)

times_square_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "swhOdQ9tCaEq"),
  iconWidth = icon_width, iconHeight = icon_height
)

bridge_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "pJmt_hefPAkD"),
  iconWidth = icon_width, iconHeight = icon_height
)

museum_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "3496"),
  iconWidth = icon_width, iconHeight = icon_height
)

monument_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "4703"),
  iconWidth = icon_width, iconHeight = icon_height
)

college_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "11173"),
  iconWidth = icon_width, iconHeight = icon_height
)

stadium_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "JlLTMd4NfTIp"),
  iconWidth = icon_width, iconHeight = icon_height
)

airport_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "3683"),
  iconWidth = icon_width, iconHeight = icon_height
) 

ferry_icon <- makeIcon(
  iconUrl = paste(icon_base_url, "PLcwTvSptrJJ"),
  iconWidth = icon_width, iconHeight = icon_height
)

room_type_icons <- iconList(
  "Private room" = makeIcon(
    iconUrl = paste(icon_base_url, "BBsnsTkuoCkC"),
    iconWidth = icon_width, iconHeight = icon_height
  ),
  "Entire home/apt" = makeIcon(
    iconUrl = paste(icon_base_url, "Uc54oOGCsjOE"),
    iconWidth = icon_width, iconHeight = icon_height
  ),
  "Shared room" = makeIcon(
    iconUrl = paste(icon_base_url, "46791"),
    iconWidth = icon_width, iconHeight = icon_height
  )
)

```


```{r}

plot_map <- function (df, zoom, ratio, showMarkers) {
  center_lng <- (df %>% summarise(mean(longitude)))[1, 1]
  center_lat <- (df %>% summarise(mean(latitude)))[1, 1]
  
  map_max <- nrow(df) * ratio
  
  base_map <- df %>%
    leaflet() %>%
    addTiles() %>%
    addProviderTiles(providers$OpenStreetMap.DE) %>%
    setView(center_lng, center_lat, zoom) %>%
    addHeatmap(lng = ~longitude, lat = ~latitude, max = map_max, radius = 20, blur = 10)
  
  if (showMarkers)
    return(
      base_map %>%
        addMarkers(lat = 40.6897, lng = -74.0445, label = "Statue of Liberty", icon = statue_of_liberty_icon) %>%
        addMarkers(lat = 40.7484, lng = -73.9856, label = "Empire State Building", icon = skyscraper_icon) %>%
        addMarkers(lat = 40.7826, lng = -73.9655, label = "Central Park", icon = park_icon) %>%
        addMarkers(lat = 40.7579, lng = -73.9855, label = "Times Square", icon = times_square_icon) %>%
        addMarkers(lat = 40.7061, lng = -73.9967, label = "Brooklyn Bridge", icon = bridge_icon) %>%
        addMarkers(lat = 40.7188, lng = -73.9900, label = "Tenement Museum", icon = museum_icon) %>%
        addMarkers(lat = 40.7794, lng = -73.9632, label = "Metropolitan Museum of Art", icon = museum_icon) %>%
        addMarkers(lat = 40.7029, lng = -74.0153, label = "The Battery", icon = park_icon) %>%
        addMarkers(lat = 40.7074, lng = -73.9907, label = "Manhattan Bridge", icon = bridge_icon) %>%
        addMarkers(lat = 40.7136, lng = -73.9719, label = "Williamsburg Bridge", icon = bridge_icon) %>%
        addMarkers(lat = 40.7129, lng = -74.0132, label = "One World Observatory", icon = skyscraper_icon) %>%
        addMarkers(lat = 40.7129, lng = -74.0132, label = "Stonewall National Monument", icon = monument_icon) %>%
        addMarkers(lat = 40.7421, lng = -73.9879, label = "Madison Square Park", icon = park_icon) %>%
        addMarkers(lat = 40.7468, lng = -73.9582, label = "Gantry Plaza State Park", icon = park_icon) %>%
        addMarkers(lat = 40.8971, lng = -73.8862, label = "Van Cortlandt Park", icon = park_icon) %>%
        addMarkers(lat = 40.8655, lng = -73.8943, label = "Edgar Allan Poe Cottage", icon = monument_icon) %>%
        addMarkers(lat = 40.8729, lng = -73.8945, label = "Lehman College, CUNY", icon = college_icon) %>%
        addMarkers(lat = 40.8774, lng = -73.8794, label = "The Museum of Bronx History", icon = museum_icon) %>%
        addMarkers(lat = 40.8295, lng = -73.9262, label = "Yankee Stadium", icon = stadium_icon) %>%
        addMarkers(lat = 40.7563, lng = -73.9240, label = "Museum of the Moving Image", icon = museum_icon) %>%
        addMarkers(lat = 40.7767, lng = -73.8739, label = "LaGuardia Airport", icon = airport_icon) %>%
        addMarkers(lat = 40.7438, lng = -73.9351, label = "LaGuardia Community College", icon = college_icon) %>%
        addMarkers(lat = 40.6467, lng = -74.0765, label = "The Staten Island September 11th Memorial", icon = monument_icon) %>%
        addMarkers(lat = 40.6454, lng = -74.0822, label = "Curtis High School", icon = college_icon) %>%
        addMarkers(lat = 40.6436, lng = -74.0719, label = "Staten Island Ferry", icon = ferry_icon) %>%
        addMarkers(lat = 40.7022, lng = -73.9958, label = "Brooklyn Bridge Park", icon = park_icon) %>%
        addMarkers(lat = 40.6711, lng = -73.9637, label = "Brooklyn Museum", icon = museum_icon) %>%
        addMarkers(lat = 40.6913, lng = -73.9752, label = "Fort Greene Park", icon = park_icon)
    )
  
  base_map
}

```


```{r}

plot_map(df, zoom = 10, ratio = 0.05, showMarkers = F)

```

### Manhattan

```{r}

manhattan <- df %>%
  filter(neighbourhood_group == "Manhattan")

plot_map(manhattan, 11.5, ratio = 0.02, showMarkers = T)

```

### Bronx

```{r}

bronx <- df %>%
  filter(neighbourhood_group == "Bronx")

plot_map(bronx, 11.5, ratio = 0.01, showMarkers = T)

```

### Queens

```{r}

queens <- df %>%
  filter(neighbourhood_group == "Queens")

plot_map(queens, 12, ratio = 0.02, showMarkers = T)

```

```{r}

staten_island <- df %>%
  filter(neighbourhood_group == "Staten Island")

plot_map(staten_island, 12, ratio = 0.05, showMarkers = T)

```

### Brooklyn

```{r}

brooklyn <- df %>%
  filter(neighbourhood_group == "Brooklyn")

plot_map(brooklyn, 12, ratio = 0.02, showMarkers = T)

```

## Geographic distribution of room types

```{r}

plot_cluster_map <- function(df, zoom) {
  center_lng <- (df %>% summarise(mean(longitude)))[1, 1]
  center_lat <- (df %>% summarise(mean(latitude)))[1, 1]
    
  df %>%
    leaflet() %>%
    addTiles() %>%
    addProviderTiles(providers$OpenStreetMap.DE) %>%
    setView(center_lng, center_lat, zoom) %>%
    addMarkers(lng = ~longitude, lat = ~latitude, clusterOptions = markerClusterOptions(), label = ~name)
}

```

```{r}

plot_room_type_map <- function(df, zoom) {
  center_lng <- (df %>% summarise(mean(longitude)))[1, 1]
  center_lat <- (df %>% summarise(mean(latitude)))[1, 1]
    
  df %>%
    leaflet() %>%
    addTiles() %>%
    addProviderTiles(providers$OpenStreetMap.DE) %>%
    setView(center_lng, center_lat, zoom) %>%
    addMarkers(
      lng = ~longitude, 
      lat = ~latitude, 
      icon = ~room_type_icons[room_type], 
      label = ~name,
      popup = ~paste(sep = "<br/>",
        paste("<b>", name, "</b>"),
        paste("Price: ", price, "$"),
        paste("#Reviews: ", number_of_reviews),
        paste("Review age: ", last_review_age, "days"),
        paste("Availability: ", availability_365, "days")
      )
    )
}

```

```{r}

highest_review <- df %>%
  arrange(desc(number_of_reviews)) %>%
  head(100)
  
plot_room_type_map(
  highest_review, 
  11
)

```

```{r}

plot_room_type_map(df %>% filter(price_bin == "0-50") %>% arrange(desc(number_of_reviews)) %>% head(500), 11)

```


```{r}

plot_room_type_map(df %>% filter(price_bin == "1001+", number_of_reviews != 0) %>% arrange(last_review_age) %>% head(500), 11)

```


```{r}

plot_room_type_map(df %>% arrange(desc(number_of_reviews)) %>% head(10), 11)

```

```{r}

plot_room_type_map(df %>% arrange(availability_365) %>% head(200), 11)

```


### Private room

```{r}

private_room <- df %>%
  filter(room_type == "Private room")

plot_cluster_map(private_room, 10)

```

### Entire home

```{r}

entire_home <- df %>%
  filter(room_type == "Entire home/apt")

plot_cluster_map(entire_home, 10)

```

### Shared room

```{r}

shared_room <- df %>%
  filter(room_type == "Shared room")

plot_cluster_map(shared_room, 10)

```



## Most expensive listing position

```{r}

plot_marker_map <- function(df, zoom, color) {
  center_lng <- (most_expensive %>% summarise(mean(longitude)))[1, 1]
  center_lat <- (most_expensive %>% summarise(mean(latitude)))[1, 1]

  df %>%
    leaflet() %>%
    addTiles() %>%
    addProviderTiles(providers$OpenStreetMap.DE) %>%
    setView(center_lng, center_lat, zoom) %>%
    addCircleMarkers(lng = ~longitude, lat= ~latitude, label = ~name, color = color)
}

```

## Cheapest listings position

```{r}

most_expensive <- df %>%
  arrange(desc(price)) %>%
  head(10)

plot_marker_map(most_expensive, 11, "red")

```

```{r}

least_expensive <- df %>%
  arrange(price) %>%
  head(10)

plot_marker_map(least_expensive, 11, "blue")

```
