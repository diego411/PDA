---
title: "Data Cleaning"
output: github_document
---

Load required packages.

```{r load packages, warning=FALSE, message=FALSE}

library( tidyverse )
library( dplyr )
library( gridExtra )

```

Set up workspace, i.e., remove all existing data from working memory and load data from CSV file.

```{r setup}

rm( list=ls() )
df <- read.csv("./data/airbnb_data.csv")

```

# Transform data

Convert the room_type, neighborhood_group, and neighborhood variables into factors and the last_review variable into a Date object.

```{r}

head(df)
df <- df %>%
  mutate(
    room_type = as.factor(room_type),
    neighbourhood_group = as.factor(neighbourhood_group),
    neighbourhood = as.factor(neighbourhood),
    last_review=as.Date(last_review, format = "%Y-%m-%d")
  )

str(df)

```


# Missing values

Count the number of missing values and calculate the percentage.
 
```{r}

empty_values <- c(NA, NULL, "", " ")

get_na_summary <- function(df) {
  nrows = nrow(df)
  NAs <- data.frame()

  for (column_name in colnames(df)) {
    na_count <- sum(df[[column_name]] %in% empty_values)#is.na(df[[column_name]]))
    row <- data.frame(
      variable = column_name,
      na_count = na_count,
      na_percent = round(na_count / nrows, 4) 
    )
    NAs <- rbind(NAs, row)
  }

  return(NAs)
}

get_na_summary(df)

```

Defining a function to filter out rows where variables contain missing values.
Missing values in reviews_per_month are replaced with 0, assuming no review means zero reviews per month. 
Keep only rows without empty values in the name and host_name variables.
Check the dataframe again for missing values after the cleaning.

```{r}

`%nin%` = Negate(`%in%`)

df <- df %>%
  mutate(reviews_per_month = ifelse(is.na(reviews_per_month), 0, reviews_per_month)) %>%
  filter(name %nin% empty_values) %>%
  filter(host_name %nin% empty_values)

get_na_summary(df)

```

# Feature Engineering

Create a variable that adds the most recent review date available in the dataset and store it in latest_review.

```{r}

latest_review <- df %>%
  filter(!is.na(last_review)) %>%
  summarise(max(last_review))
latest_review <- latest_review[1,]

```

Create a variable that contains the distance to the "center" of New York in Manhattan, here defined as the Columbus Circle.

```{r}
center_latitude <- 40.767811385445356
center_longitude <- -73.98156481716236

manhatten_distance <- function(x1, y1, x2, y2) {
  return(abs(x1 - x2) + abs(y1 - y2))  
}

```

Create a variable that indicates the time passed since the last review. Another variable reflects the distance from the Columbus Circle in Manhattan.
Add both to the dataframe.

```{r}

df <- df %>%
  mutate(
    last_review_age = latest_review - last_review,
    distance_from_center = manhatten_distance(center_latitude, center_longitude, latitude, longitude)  
  )

head(df)

```

Generate kernel density estimates for each variable in the dataframe to assess their distributions.

```{r}

plot_distributions <- function(df) {
  plots <- lapply(names(df), function(var) {
    ggplot(df, aes(x = df[[var]])) +
      geom_density() + # You can also use geom_histogram() for histograms
      labs(title = var)
  })
  grid.arrange(grobs = plots, ncol = 2)
}

df_sample <- df %>%
  select(latitude, longitude, price, number_of_reviews, reviews_per_month, calculated_host_listings_count,  distance_from_center)

plot_distributions(df_sample)

```

Log transforme skewed distributions.

```{r}

df_log <- df %>%
  mutate(
    log_price = log(price),
    log_number_of_reviews = log(number_of_reviews),
    log_reviews_per_month = log(reviews_per_month),
    log_calculated_host_listings_count = log(calculated_host_listings_count)
  )

df_sample <- df_log %>%
  select(
    latitude, longitude, log_price, log_number_of_reviews, log_reviews_per_month, log_calculated_host_listings_count, distance_from_center
  )

plot_distributions(df_sample)

```

Save the modified data to the dataframe.

```{r}

df <- df_log

```


Bin price and check the distribution across bins.

```{r}

bins <- c(0, 50, 100, 200, 500, 1000, 10000)
labels <- c("0-50", "51-100", "101-200", "201-500", "501-1000", "1001+")
df <- df %>%
  mutate(price_bin = cut(price, breaks = bins, labels = labels, include.lowest = TRUE, right = FALSE))

df %>%
  count(price_bin) %>%
  ggplot(aes(x = price_bin, y = n)) +
  geom_col() +
  labs(title="Distribution of Price Bins", x="Price Bins", y="Frequency")

```

The listings are distributed across the bins, though there is less data for the two highest price categories.
Most listings are in bins 2 and 3, i.e., between $51 and $500. The adjacent bins 1 and 4 are also well-represented.

Create a variable that counts the characters in a name.

```{r}

df <- df %>%
  mutate(name_length = nchar(name))

```


# Consistency

## Min/max

Summarize the data.

```{r}

df_numeric <- df %>%
  select(
    latitude, longitude, price, minimum_nights, number_of_reviews, last_review, reviews_per_month, 
    calculated_host_listings_count, availability_365, last_review_age, distance_from_center
  )

summary(df_numeric)

```

All numeric variables (except longitude) have non-negative values. Max for availability_365 is 365.


## Price

Price: Minimum value of 0, likely indicating missing data rather than actual free listings.
Minimum Nights: A maximum of 1250 nights suggests potential outliers.

Check entries with price equal to 0.

```{r}

df %>%
  filter(price == 0)

```

Since there are only 11 listings with a price of zero, we assume this is a data gathering error and remove the respective rows.

```{r}

df <- df_non_zero_price <- df %>% 
  filter(price != 0)

```

## Minimum nights

```{r}

max(df$minimum_nights)

```

The maximum for the variable minimum_nights is 1250, meaning that particular listing would have to be rented as a minimum for well over 3 years. This seems excessive as AirBnB in most cases is used for holidays or short to mid stays. We do not see a reason for a host to require the listing to be rented for such a long time and thus will consider it and similar entries as data anomalies.


```{r}

quantile(df$minimum_nights, 0.9999)

```

The minimum required nights is less than 500 for 99.99% of all listing. This number seems more or less reasonable. In the following we will therefore remove all entries that fall above this threshold.

```{r}

df <- df %>%
  filter(minimum_nights <= quantile(minimum_nights, 0.9999))

```


## Unique ids

Compare the number of rows and the number of unique ids to determine whether each row has a unique identifier.

```{r}

length_df <- nrow(df)
unique_ids <- length(unique(df$id))

length_df == unique_ids

```
There are no id inconsistencies.

## Reviews

Filter the dataframe to find entries where reviews_per_month is 0 but number_of_reviews is not 0, find entries with 0 number_of_reviews but a non-NA last_review date and identify rows where reviews_per_month is 0 but last_review is not NA.

```{r}

result <- df %>%
  filter(
    reviews_per_month == 0, 
    number_of_reviews != 0
  )

nrow(result)

result <- df %>%
  filter(
    number_of_reviews == 0,
    !is.na(last_review)
  )

nrow(result)

result <- df %>%
  filter(
    reviews_per_month == 0,
    !is.na(last_review)
  )

nrow(result)

```
All review variables are internally consistent.

# Duplicates

Find duplicates based on the id column and identify listings that are at the exact same location.

```{r}

get_duplicates_by_columns <- function(df, column_names) {
  column_indexes <- unlist(lapply(column_names, function(name) {
    return (grep(name, colnames(df)))
  })) 
  
  return (df[duplicated(df[,column_indexes]) | duplicated(df[,column_indexes], fromLast = TRUE),])
}

```

## Duplicate ids

```{r}

nrow(get_duplicates_by_columns(df, c("id")))

```
## Duplicate names

```{r}

get_duplicates_by_columns(df, c("name")) %>%
  select(id, name, host_name, neighbourhood, price) %>%
  arrange(desc(name))

```
There are a lot of duplicate names in the dataset. Most of the time these listings seem to be from the same host. However, the listings seem to always differ in the neighbourhood or in the price. This will be further analysed in the following.

```{r}

get_duplicates_by_columns(df, c("name", "latitude", "longitude")) %>%
  select(id, name, host_name, price, latitude, longitude) %>%
  arrange(desc(name))

```

As one can see there are no listings with the same name that are also at the same geographic position. Since the combination of name, latitude and longitude is already a super key differentiating every row, no furher supersets of this combination need to be analysed.

## Duplicate positions

```{r}

get_duplicates_by_columns(df, c("longitude", "latitude")) %>%
  select(id, name, host_name, longitude, latitude) %>%
  arrange(desc(latitude))

```
There are some listings that are at the exact same position. However, they more often than not have a different host or name and in every case a different price. Thus, we will not drop these entries and consider them in our analysis.

# Export

```{r}

write.csv(df, "./data/airbnb_clean.csv")

```

